# seed: 42
# task_type: sft

dataset:
  train_data_path: /workspace/ye/dataset/prm800k-main/prm800k/math_splits/train.jsonl
  val_data_path: /workspace/ye/dataset/prm800k-main/prm800k/math_splits/test.jsonl
  test_data_path: /workspace/ye/dataset/prm800k-main/prm800k/math_splits/test.jsonl
#/workspace/ye/AI_FeedBack/data/hh_rlhf/helpful_base_en_train.jsonl
#/workspace/ye/AI_FeedBack/data/hh_rlhf/helpful_base_en_test.jsonl
  max_source_length: 512
  max_target_length: 768
  data_type: math

model:
  model_type: 'mistral'
  offload: false
  model_path: /workspace/ye/llm/mistral-7b
#/workspace/ye/llm/chatglm3-6b-base
#/workspace/ye/llm/opt-1b3
train:
  gradient_accumulation_steps: 1
  learning_rate: 1e-5
  lr_scheduler_type: cosine
  per_device_eval_batch_size: 2
  per_device_train_batch_size: 2
  num_warmup_steps: 100
  weight_decay: 0.1
  num_train_epochs: 3
  Dropout: 0.1
  save_path: /workspace/ye/SFT/checkpoint
  beta: 0.5
  lora_version: ''
  lora_rank: 8
  scheduler: CAWR
  T_mult: 1
  rewarm_epoch_num : 1

deepspeed:
  offload: false
  zero_stage: 2

log:
  checkpoint_save_interval: 1000
  eval_epoch_ratio: 0.2
  eval_interval: 1000
  project_name: 'SFT'
  run_name: '3045-lr1e5-mistral-math'
  output_dir: /workspace/ye/SFT/checkpoint

evaluator:
  data_path: ''
  checkpoint_path: ''
  result_save_path: ''